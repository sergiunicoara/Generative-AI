# LinkedIn Post â€” copy-paste ready (no markdown, renders correctly on LinkedIn)

---

Most vector DB benchmarks are measuring a fantasy.

They test on random vectors. Production RAG systems run on real text â€” clustered, overlapping, and full of hard negatives.

I ran a full stress test on 4 engines (Qdrant, Elasticsearch, Redis, pgvector) using 20,000 real Wikipedia embeddings.

Here's what actually matters in production.

All tests ran on the same hardware, warm caches, fixed embedding model, and identical HNSW parameters unless stated otherwise.

â€•â€•â€•â€•â€•â€•â€•â€•â€•â€•â€•â€•â€•â€•â€•â€•â€•â€•â€•â€•â€•â€•

ğŸš€ Speed under load
(single node, 20k vectors, independent clients)

Redis peaks at 918 req/sec.
pgvector reaches 164.
Qdrant 120.
Elasticsearch 61 â€” on a single node.

Yes, Elasticsearch scales horizontally. But most small-to-mid RAG deployments start on one node â€” and that's where Redis wins without extra infrastructure.

One thing that surprised me: pgvector's single-thread latency dropped from ~58ms to 18ms once each client used a persistent connection instead of reconnecting per query.
The old number was penalising connection overhead, not search quality.

â€•â€•â€•â€•â€•â€•â€•â€•â€•â€•â€•â€•â€•â€•â€•â€•â€•â€•â€•â€•â€•â€•

ğŸ¯ Will it find the right answer?

On fake benchmark data, all engines hit 100% recall.

On real data? Redis and pgvector cap at ~99.8% recall under standard HNSW build params (m=16, ef_construction=100).

Real topics create "hard negatives" that are genuinely difficult to resolve without rebuilding the index with higher-quality params.

â†’ That's real users occasionally getting the wrong document â€” worth knowing before you go live.

â€•â€•â€•â€•â€•â€•â€•â€•â€•â€•â€•â€•â€•â€•â€•â€•â€•â€•â€•â€•â€•â€•

âœï¸ What happens when your index gets updated?

Every production RAG system has continuous writes â€” new documents, deletions, updates.

At our test scale (10k â†’ 15k vectors with deletes), Elasticsearch latency jumped +44%.

Redis and Qdrant actually got faster â€” their internal graph rebalancing improved traversal at this size.

â†’ This is the test nobody runs before going live.

â€•â€•â€•â€•â€•â€•â€•â€•â€•â€•â€•â€•â€•â€•â€•â€•â€•â€•â€•â€•â€•â€•

ğŸ” Using a reranker (cross-encoder)?

With a 50ms reranker in the pipeline:
â€¢ Redis: 55ms end-to-end
â€¢ Elasticsearch: 158ms end-to-end

When your reranker is slow (â‰¥30ms), it dominates the latency budget. Fast retrieval still matters â€” it compounds.

If you swap in a 5ms reranker, engine choice suddenly matters a lot more again.

â€•â€•â€•â€•â€•â€•â€•â€•â€•â€•â€•â€•â€•â€•â€•â€•â€•â€•â€•â€•â€•â€•

The practical takeaway

â€¢ High traffic, single node, corpus fits in RAM?
â†’ Redis â€” blazing fast, minimal infra (watch memory pressure as you scale)

â€¢ Need the best recall + reliability?
â†’ Qdrant â€” strong quality on real data, stable under index churn

â€¢ Already running Postgres?
â†’ pgvector â€” zero new infrastructure, fragmentation-neutral; a reranker closes most of the latency gap

â€¢ Already on the Elastic stack (multi-node)?
â†’ Elasticsearch â€” throughput scales out, but watch write fragmentation per shard

â€¢ Adding a reranker â‰¥30ms?
â†’ Retrieval speed still compounds â€” don't ignore it

â€•â€•â€•â€•â€•â€•â€•â€•â€•â€•â€•â€•â€•â€•â€•â€•â€•â€•â€•â€•â€•â€•

Full benchmark open source on GitHub: [link]

What vector DB are you running in production? Curious what tradeoffs you've hit ğŸ‘‡

#RAG #vectordatabases #LLM #AIengineering #machinelearning
